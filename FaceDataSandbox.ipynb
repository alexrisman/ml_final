{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Familiar libraries.\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import cross_validation\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import svm\n",
    "import time\n",
    " \n",
    "\n",
    "# Take a moment to install Theano.  We will use it for building neural networks.\n",
    "import theano \n",
    "from theano import tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "print theano.config.device # We're using CPUs (for now)\n",
    "print theano.config.floatX # Should be 64 bit for CPUs\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_data = np.genfromtxt(\"training.csv\", delimiter=\",\", skip_header=1, usecols=(30)\n",
    "                           , converters={30: lambda x : x.replace(\" \", \",\")})\n",
    "train_data = np.array(map(lambda x : np.array(x.split(\",\")).astype(int), train_data))              \n",
    "train_labels = np.genfromtxt(\"training.csv\", delimiter=\",\", skip_header=1, usecols=np.arange(30))\n",
    "shuffle = np.random.permutation(np.arange(len(train_data)))\n",
    "train_data, train_labels = train_data[shuffle], train_labels[shuffle]\n",
    "test_data, test_labels = train_data[6000:], train_labels[6000:]\n",
    "train_data, train_labels = train_data[:1000], train_labels[:1000]\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "train_l_eye_center_x, test_l_eye_center_x = train_labels[:,1], test_labels[:,1]\n",
    "c1 = ~np.isnan(train_l_eye_center_x)\n",
    "c2 = ~np.isnan(test_l_eye_center_x)\n",
    "train_l_eye_center_x, test_l_eye_center_x = train_l_eye_center_x[c1], test_l_eye_center_x[c2]\n",
    "train_data_l_eye_x, test_data_l_eye_x = train_data[c1], test_data[c2]\n",
    "print \"done\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting\n",
      "done fitting\n",
      "0.0253662183391\n",
      "[ 35.85741972  38.40062136  40.06935652  37.03348966  38.81603478\n",
      "  37.60270439  34.64367724  38.29228615  37.66240569  38.68093585]\n",
      "[ 37.33464166  37.84373053  41.31575921  36.65311362  39.94004226\n",
      "  31.58072542  34.01099241  36.76979507  40.03090236  35.82455776]\n",
      "3.19683970631\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "print \"start fitting\"\n",
    "lr.fit(train_data_l_eye_x, train_l_eye_center_x)\n",
    "print \"done fitting\"\n",
    "preds = lr.predict(test_data_l_eye_x)\n",
    "print lr.score(test_data_l_eye_x, test_l_eye_center_x)\n",
    "print test_l_eye_center_x[:10]\n",
    "print preds[:10]\n",
    "print np.sqrt(mean_squared_error(test_l_eye_center_x, preds))\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -1.22510959e+21,  -1.32272154e+20])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_validation.cross_val_score(LinearRegression(), train_data_l_eye_x, y=train_l_eye_center_x, scoring='root_mean_squared_error', cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False), LinearRegression(copy_X=True, fit_intercept=True, normalize=False)]\n"
     ]
    }
   ],
   "source": [
    "train_data, train_labels = train_data[:2000], train_labels[:2000]\n",
    "feature_lrs = []\n",
    "i = 0\n",
    "while i < 30:\n",
    "    feature_train_labels = train_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_train_labels)\n",
    "    feature_train_labels = feature_train_labels[remove_nan]\n",
    "    feature_train_data = train_data[remove_nan]\n",
    "    feature_lr = LinearRegression()\n",
    "    feature_lr.fit(feature_train_data, feature_train_labels)\n",
    "    feature_lrs.append(feature_lr)\n",
    "    i += 1\n",
    "    \n",
    "print feature_lrs\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.4203886232471703, 3.1968397063149112, 4.2862249439474276, 3.1955835531876322, 2.0726150670760624, 1.7969938165709884, 2.9099751087269778, 2.28886112014274, 2.5992171935264734, 1.827160626688296, 3.1753097212102461, 2.2332240919455635, 3.0601995390057537, 2.7236461357891226, 2.6147127653263307, 3.2500418290126922, 3.110722752397713, 2.730710604170226, 3.2320196556391667, 3.2003935435194761, 5.0799806807228167, 6.4531159171281258, 3.3852377119142876, 3.8839393517979452, 3.7104300601205602, 3.8419574508393786, 3.1286389401652128, 3.8080587545984765, 5.0991290140629628, 6.1987427687862136]\n",
      "3.41713570159\n"
     ]
    }
   ],
   "source": [
    "feature_rmses = []\n",
    "i = 0\n",
    "while i < 30:\n",
    "    feature_test_labels = test_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_test_labels)\n",
    "    feature_test_labels = feature_test_labels[remove_nan]\n",
    "    feature_test_data = test_data[remove_nan]\n",
    "    feature_lr = feature_lrs[i]\n",
    "    feature_preds = feature_lr.predict(feature_test_data)\n",
    "    feature_rmse = np.sqrt(mean_squared_error(feature_test_labels, feature_preds))\n",
    "    feature_rmses.append(feature_rmse)\n",
    "    i += 1\n",
    "    \n",
    "print feature_rmses\n",
    "print np.mean(feature_rmses)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "print len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tNeuralNetwork(training_data, training_labels, testing_data):\n",
    "    # Try playing with this value.\n",
    "    numHiddenNodes = 600 \n",
    "    numFeatures = training_data[1].size\n",
    "    w_1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "    w_2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, 1))*.01)))\n",
    "    params = [w_1, w_2]\n",
    "\n",
    "\n",
    "    ## (2) Model\n",
    "    X = T.matrix(\"X\")\n",
    "    Y = T.vector(\"Y\")\n",
    "    # Two notes:\n",
    "    # First, feed forward is the composition of layers (dot product + activation function)\n",
    "    # Second, activation on the hidden layer still uses sigmoid\n",
    "    def model(X, w_1, w_2):\n",
    "        return T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2)\n",
    "    y_hat = model(X, w_1, w_2)\n",
    "    \n",
    "\n",
    "    ## (3) Cost...same as linear regression\n",
    "    cost = T.mean((y_hat - Y)**2)\n",
    "\n",
    "    ## (4) Minimization.  Update rule changes to backpropagation.\n",
    "    alpha = 0.0001\n",
    "    \n",
    "    def backprop(cost, w):\n",
    "        grads = T.grad(cost=cost, wrt=w)\n",
    "        updates = []\n",
    "        for w1, grad in zip(w, grads):\n",
    "            updates.append([w1, w1 - grad * alpha])\n",
    "        return updates\n",
    "    \n",
    "    update = backprop(cost, params)\n",
    "    train = theano.function(inputs=[X, Y], outputs=cost, updates=update, allow_input_downcast=True)\n",
    "    predict = theano.function(inputs=[X], outputs=y_hat, allow_input_downcast=True)\n",
    "\n",
    "    miniBatchSize = 1 \n",
    "    def gradientDescentStochastic(epochs, training_data, training_labels):\n",
    "        for i in range(epochs):\n",
    "            for start, end in zip(range(0, len(training_data), miniBatchSize), range(miniBatchSize, len(training_data), miniBatchSize)):\n",
    "                cost = train(training_data[start:end], training_labels[start:end])\n",
    "\n",
    "\n",
    "    gradientDescentStochastic(5, training_data, training_labels)\n",
    "    return predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.9753809657051464, 3.9251387419157475]\n"
     ]
    }
   ],
   "source": [
    "mini_train_data, mini_train_labels = train_data[:100]/255.0, train_labels[:100]/255.0\n",
    "feature_pred_list = []\n",
    "feature_rmses = []\n",
    "i = 0\n",
    "while i < 2:\n",
    "    feature_train_labels = mini_train_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_train_labels)\n",
    "    feature_train_labels = feature_train_labels[remove_nan]\n",
    "    feature_train_data = mini_train_data[remove_nan]\n",
    "    feature_test_labels = test_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_test_labels)\n",
    "    feature_test_labels = feature_test_labels[remove_nan]\n",
    "    feature_test_data = test_data[remove_nan]\n",
    "    feature_preds = tNeuralNetwork(feature_train_data, feature_train_labels, feature_test_data/255.0)\n",
    "    feature_rmse = np.sqrt(mean_squared_error(feature_test_labels, feature_preds*255.0))\n",
    "    feature_rmses.append(feature_rmse)\n",
    "    feature_pred_list.append(feature_preds)\n",
    "    i += 1\n",
    "    \n",
    "print feature_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.41568627  0.3254902   0.29019608 ...,  0.18039216  0.19607843\n",
      "   0.21960784]\n",
      " [ 0.09411765  0.08235294  0.06666667 ...,  0.34117647  0.3372549\n",
      "   0.34117647]\n",
      " [ 0.19215686  0.19607843  0.2        ...,  0.40784314  0.40392157\n",
      "   0.41176471]\n",
      " ..., \n",
      " [ 0.37254902  0.09019608  0.01176471 ...,  0.21568627  0.50196078\n",
      "   0.48235294]\n",
      " [ 0.32156863  0.38431373  0.38039216 ...,  0.6745098   0.7254902\n",
      "   0.78431373]\n",
      " [ 0.21176471  0.21568627  0.23921569 ...,  0.40392157  0.40392157\n",
      "   0.40392157]]\n"
     ]
    }
   ],
   "source": [
    "print train_data / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tCntxtNeuralNetwork(training_data, training_labels, testing_data):\n",
    "    # Try playing with this value.\n",
    "    numHiddenNodes = 600 \n",
    "    numFeatures = training_data[1].size\n",
    "    numLabels = training_labels[1].size\n",
    "    pred_dict = {}\n",
    "    weight_dict = {}\n",
    "    pred_fun_dict = {}\n",
    "    train_fun_dict = {}\n",
    "    params = [w_1, w_2]\n",
    "    for i in np.arange(numLabels):\n",
    "        curr_w1 = theano.shared(np.asarray((np.random.randn(*(numFeatures, numHiddenNodes))*.01)))\n",
    "        curr_w2 = theano.shared(np.asarray((np.random.randn(*(numHiddenNodes, 1))*.01)))\n",
    "        weight_dict[i] = [curr_w1, curr_w2]        \n",
    "        curr_pred_fun = theano.function(inputs=[X, Y], outputs=y_hat\n",
    "                                        , updates=[(weight_dict[i][0], allow_input_downcast=True)\n",
    "\n",
    "    ## (2) Model\n",
    "    X = T.matrix(\"X\")\n",
    "    Y = T.vector(\"Y\")\n",
    "#     w_1 = T.matrix(\"w_1\")\n",
    "#     w_2 = T.vector(\"w_2\")\n",
    "    # Two notes:\n",
    "    # First, feed forward is the composition of layers (dot product + activation function)\n",
    "    # Second, activation on the hidden layer still uses sigmoid\n",
    "    def model(X, w_1, w_2):\n",
    "        return T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2)\n",
    "    y_hat = model(X, w_1, w_2)\n",
    "    \n",
    "\n",
    "    ## (3) Cost...same as linear regression\n",
    "    cost = T.mean((y_hat - Y)**2)\n",
    "\n",
    "    ## (4) Minimization.  Update rule changes to backpropagation.\n",
    "    alpha = 0.0001\n",
    "    \n",
    "    def backprop(cost, w):\n",
    "        grads = T.grad(cost=cost, wrt=w)\n",
    "        updates = []\n",
    "        for w1, grad in zip(w, grads):\n",
    "            updates.append([w1, w1 - grad * alpha])\n",
    "        return updates\n",
    "    \n",
    "    update = backprop(cost, params)\n",
    "    train = theano.function(inputs=[X, Y], outputs=y_hat, updates=update, allow_input_downcast=True)\n",
    "    predict = theano.function(inputs=[X], outputs=y_hat, allow_input_downcast=True)\n",
    "\n",
    "    miniBatchSize = 1 \n",
    "    def gradientDescentStochastic(epochs, training_data, training_labels):\n",
    "        for i in range(epochs):\n",
    "            for start, end in zip(range(0, len(training_data), miniBatchSize), range(miniBatchSize, len(training_data), miniBatchSize)):\n",
    "                for i in np.arange(numLabels):\n",
    "                    pred = train(training_data[start:end], training_labels[start:end])\n",
    "\n",
    "\n",
    "    gradientDescentStochastic(5, training_data, training_labels)\n",
    "    return predict(testing_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NNet:\n",
    "    X = T.matrix(\"X\")\n",
    "    Y = T.vector(\"Y\")\n",
    "    def model(self, X, w_1, w_2):\n",
    "        return T.dot(T.nnet.sigmoid(T.dot(X, w_1)), w_2)\n",
    "    \n",
    "    def backprop(self, cost, w):\n",
    "        grads = T.grad(cost=cost, wrt=w)\n",
    "        updates = []\n",
    "        for w1, grad in zip(w, grads):\n",
    "            updates.append([w1, w1 - grad * self.alpha])\n",
    "        return updates\n",
    "    \n",
    "    def __init__(self, feature_cnt, hidden_node_cnt):\n",
    "        self.numFeatures = feature_cnt\n",
    "        self.numHiddenNodes = hidden_node_cnt \n",
    "        self.w_1 = theano.shared(np.asarray((np.random.randn(*(self.numFeatures, self.numHiddenNodes))*.01)))\n",
    "        self.w_2 = theano.shared(np.asarray((np.random.randn(*(self.numHiddenNodes, 1))*.01)))\n",
    "        self.params = [self.w_1, self.w_2]\n",
    "        self.y_hat = self.model(self.X, self.w_1, self.w_2)\n",
    "        self.cost = T.mean((self.y_hat - self.Y)**2)\n",
    "        self.alpha = 0.0001\n",
    "        self.update = self.backprop(self.cost, self.params)\n",
    "        self.train = theano.function(inputs=[self.X, self.Y]\n",
    "                                     , outputs=self.y_hat, updates=self.update, allow_input_downcast=True)\n",
    "        self.predict = theano.function(inputs=[self.X], outputs=self.y_hat, allow_input_downcast=True)\n",
    "        self.miniBatchSize = 1 \n",
    "        \n",
    "    def gradientDescentStochastic(self, epochs, training_data, training_labels):\n",
    "        for i in range(epochs):\n",
    "            for start, end in zip(range(0, len(training_data), self.miniBatchSize), range(self.miniBatchSize, len(training_data), self.miniBatchSize)):\n",
    "                pred = self.train(training_data[start:end], training_labels[start:end])\n",
    "\n",
    "    def fit(self, training_data, training_labels):\n",
    "        self.gradientDescentStochastic(5, training_data, training_labels)\n",
    "        \n",
    "    def pred(self, testing_data):\n",
    "        return self.predict(testing_data)\n",
    "    \n",
    "    def fit_step(self, training_data, training_labels, start, end):\n",
    "        pred = self.train(training_data[start:end], training_labels[start:end])\n",
    "        return pred\n",
    "        \n",
    "\n",
    "        \n",
    "def mutualTrain(training_data, training_labels, epochs):\n",
    "    nnets = []\n",
    "    net_cnt = training_labels.shape[1]\n",
    "    preds = np.zeros(net_cnt)\n",
    "    aug_cnt = net_cnt\n",
    "    img_size = training_data.shape[1]\n",
    "    aug_training_data = np.zeros((training_data.shape[0], img_size + aug_cnt))\n",
    "    aug_training_data[:,:-aug_cnt] = training_data\n",
    "    for i in range(net_cnt):\n",
    "        nnet = NNet(img_size + aug_cnt, 600)\n",
    "        nnets.append(nnet)\n",
    "    for i in range(epochs):\n",
    "        for start, end in zip(range(0, len(aug_training_data), 1), range(1, len(aug_training_data), 1)):\n",
    "            for j in range(net_cnt):\n",
    "                curr_net = nnets[j]\n",
    "                pred = curr_net.fit_step(aug_training_data, training_labels[j], start, end)\n",
    "                aug_training_data[start, j] = pred\n",
    "    return nnets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00179916  0.01344482  0.00464214 ...,  0.00712863  0.00613196\n",
      "   0.01374534]\n",
      " [-0.00015212 -0.00772643  0.00182587 ..., -0.00962728  0.00681799\n",
      "   0.00433796]\n",
      " [ 0.00294586  0.00269758  0.01145068 ..., -0.00484326 -0.0016041\n",
      "   0.01242837]\n",
      " ..., \n",
      " [-0.00131788  0.00344589 -0.00263903 ..., -0.00421758 -0.00702426\n",
      "   0.00293342]\n",
      " [-0.00814809  0.01645486 -0.01164123 ..., -0.00438224 -0.00650956\n",
      "  -0.0031884 ]\n",
      " [-0.01905902  0.00737533 -0.00758742 ...,  0.00774335 -0.01165548\n",
      "   0.00312265]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-271-2975afed3520>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfeature_nn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mprint\u001b[0m \u001b[0mfeature_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mfeature_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_train_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m#     print feature_nn.w_1.eval()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mfeature_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_test_data\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-269-2d6063df381d>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, training_data, training_labels)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradientDescentStochastic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-269-2d6063df381d>\u001b[0m in \u001b[0;36mgradientDescentStochastic\u001b[0;34m(self, epochs, training_data, training_labels)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminiBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminiBatchSize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminiBatchSize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;31m#                 print pred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mini_train_data, mini_train_labels = train_data[:100]/255.0, train_labels[:100]/255.0\n",
    "feature_pred_list = []\n",
    "feature_rmses = []\n",
    "i = 0\n",
    "while i < 1:\n",
    "    feature_train_labels = mini_train_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_train_labels)\n",
    "    feature_train_labels = feature_train_labels[remove_nan]\n",
    "    feature_train_data = mini_train_data[remove_nan]\n",
    "    feature_test_labels = test_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_test_labels)\n",
    "    feature_test_labels = feature_test_labels[remove_nan]\n",
    "    feature_test_data = test_data[remove_nan]\n",
    "    feature_nn = NNet(feature_train_data[0].size, 600)\n",
    "    print feature_nn.w_1.eval()\n",
    "    feature_nn.fit(feature_train_data, feature_train_labels)\n",
    "#     print feature_nn.w_1.eval()\n",
    "    feature_preds = feature_nn.pred(feature_test_data/255.0)\n",
    "    feature_rmse = np.sqrt(mean_squared_error(feature_test_labels, feature_preds*255.0))\n",
    "    feature_rmses.append(feature_rmse)\n",
    "    feature_pred_list.append(feature_preds)\n",
    "    i += 1\n",
    "    \n",
    "print feature_rmses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input dimension mis-match. (input[1].shape[1] = 1, input[3].shape[1] = 0)\nApply node that caused the error: Elemwise{Composite{[mul(i0, i1, sub(i2, i3))]}}[(0, 1)](TensorConstant{(1, 1) of 2.0}, Alloc.0, Dot22.0, InplaceDimShuffle{x,0}.0)\nInputs shapes: [(1, 1), (1, 1), (1, 1), (1, 0)]\nInputs strides: [(8, 8), (8, 8), (8, 8), (0, 8)]\nInputs types: [TensorType(float64, (True, True)), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, row)]\nUse the Theano flag 'exception_verbosity=high' for a debugprint of this apply node.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-283-360dcf5150e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmini_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_train_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmutualTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmini_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_train_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"done\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-282-220578b9c794>\u001b[0m in \u001b[0;36mmutualTrain\u001b[0;34m(training_data, training_labels, epochs)\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_cnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mcurr_net\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnnets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                 \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurr_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maug_training_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m                 \u001b[0maug_training_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnnets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-282-220578b9c794>\u001b[0m in \u001b[0;36mfit_step\u001b[0;34m(self, training_data, training_labels, start, end)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfit_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpred\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    586\u001b[0m                     \u001b[0;31m# For the CVM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m                     gof.vm.raise_with_op(self.fn.nodes[self.fn.position_of_error],\n\u001b[0;32m--> 588\u001b[0;31m                                          self.fn.thunks[self.fn.position_of_error])\n\u001b[0m\u001b[1;32m    589\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m                     \u001b[0;31m# For the c linker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input dimension mis-match. (input[1].shape[1] = 1, input[3].shape[1] = 0)\nApply node that caused the error: Elemwise{Composite{[mul(i0, i1, sub(i2, i3))]}}[(0, 1)](TensorConstant{(1, 1) of 2.0}, Alloc.0, Dot22.0, InplaceDimShuffle{x,0}.0)\nInputs shapes: [(1, 1), (1, 1), (1, 1), (1, 0)]\nInputs strides: [(8, 8), (8, 8), (8, 8), (0, 8)]\nInputs types: [TensorType(float64, (True, True)), TensorType(float64, matrix), TensorType(float64, matrix), TensorType(float64, row)]\nUse the Theano flag 'exception_verbosity=high' for a debugprint of this apply node."
     ]
    }
   ],
   "source": [
    "mini_train_data, mini_train_labels = train_data[:100]/255.0, train_labels[:100,:2]/255.0\n",
    "nns = mutualTrain(mini_train_data, mini_train_labels, 1)\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6607873191580929, 3.2579264067701872, 3.3218165342085619, 3.1732604675076681, 2.1265198480384475, 2.1718428639253151, 3.5623513967445439, 3.0441125185429962, 2.3213314677281733, 2.1752059402386195, 2.9836179379488028, 3.0511166883839196, 2.9767028866344667, 2.8858544049261079, 3.8660327259073251, 3.9450146416271883, 3.3551827528498044, 2.9594311624761693, 3.4495423159795195, 4.1534258330480824, 4.4279779065832567, 5.8103364635292643, 4.048366945376876, 4.8050879967601041, 4.0301931220832863, 4.613371578671611, 3.2919278667510241, 5.3135666221849585, 4.3466421976372862, 5.4232686201887086]\n",
      "[3.612652580374188, 3.2480139709701406, 3.3285485319152626, 3.1178485773102689, 2.1112482199189757, 2.2118222097370497, 3.3403449429404311, 3.1011868448811226, 2.2552604767951316, 2.1354881636579526, 2.9986754353970264, 3.0213031783943443, 2.9397921583643831, 2.8540997063329416, 3.7869883211129522, 3.9708331241079033, 3.1800832472522047, 2.9315744405624713, 3.4648756125920457, 4.1109196977338893, 4.4381505402247026, 5.8161432980673728, 3.9588609440139577, 4.7947228894253726, 4.0286362962632216, 4.5589437490112434, 3.2578500924825322, 5.2010007004815986, 4.3519003115221002, 5.4513381436921611]\n"
     ]
    }
   ],
   "source": [
    "mini_train_data, mini_train_labels = train_data[:100], train_labels[:100]\n",
    "feat_num = 30\n",
    "train_row_cnt = mini_train_data.shape[0]\n",
    "test_row_cnt = test_data.shape[0]\n",
    "feature_nnets = []\n",
    "feature_nnets_cntxt = []\n",
    "feature_preds_train = np.zeros((train_row_cnt, feat_num))\n",
    "feature_preds_test = np.zeros((test_row_cnt, feat_num))\n",
    "feature_preds_test_cntxt = np.zeros((test_row_cnt, feat_num))\n",
    "feature_rmses_indpndt = []\n",
    "\n",
    "i = 0\n",
    "while i < feat_num:\n",
    "    feature_train_labels = mini_train_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_train_labels)\n",
    "    feature_train_labels = feature_train_labels[remove_nan]\n",
    "    feature_train_data = mini_train_data[remove_nan]\n",
    "    feature_test_labels = test_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_test_labels)\n",
    "    feature_test_labels = feature_test_labels[remove_nan]\n",
    "    feature_test_data = test_data[remove_nan]\n",
    "    feature_nn = NNet(feature_train_data[0].size, 600)\n",
    "    feature_nn.fit(feature_train_data, feature_train_labels)\n",
    "    pred_train = feature_nn.pred(mini_train_data)\n",
    "    pred_test = feature_nn.pred(test_data)\n",
    "    feature_pred_test = pred_test[remove_nan]\n",
    "    feature_rmse_indpndt = np.sqrt(mean_squared_error(feature_test_labels, feature_pred_test))\n",
    "    feature_nnets.append(feature_nn)\n",
    "    feature_preds_train[:,i] = pred_train[:,0]\n",
    "    feature_preds_test[:,i] = pred_test[:,0]\n",
    "    feature_rmses_indpndt.append(feature_rmse_indpndt)\n",
    "    i += 1\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.6607873191580929, 3.2579264067701872, 3.3218165342085619, 3.1732604675076681, 2.1265198480384475, 2.1718428639253151, 3.5623513967445439, 3.0441125185429962, 2.3213314677281733, 2.1752059402386195, 2.9836179379488028, 3.0511166883839196, 2.9767028866344667, 2.8858544049261079, 3.8660327259073251, 3.9450146416271883, 3.3551827528498044, 2.9594311624761693, 3.4495423159795195, 4.1534258330480824, 4.4279779065832567, 5.8103364635292643, 4.048366945376876, 4.8050879967601041, 4.0301931220832863, 4.613371578671611, 3.2919278667510241, 5.3135666221849585, 4.3466421976372862, 5.4232686201887086]\n",
      "[3.7594844704556745, 3.2714815050065225, 3.3231320333040184, 3.1916107019892817, 2.1071531091457816, 2.2113651863534036, 3.3612177773276333, 3.1016269813072452, 2.2595738398670728, 2.1472615938881798, 3.0122372121534862, 3.0980216300146211, 2.9755852260287452, 2.8761032747660797, 3.7985187369471545, 3.9688689529699386, 3.3143985604803348, 2.929345110285527, 3.4641458688629694, 4.1496719525001708, 4.4367337834324827, 5.7803887606832456, 3.9390728537642659, 4.7946651085299674, 4.0513774349012106, 4.5563600258593597, 3.2602336162269552, 5.2321267778431819, 4.3596759413616484, 5.4553960574320515]\n",
      "3.61839384775\n",
      "3.60622780279\n"
     ]
    }
   ],
   "source": [
    "feature_rmses_cntxt = []\n",
    "i = 0\n",
    "while i < feat_num:\n",
    "    feature_train_labels = mini_train_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_train_labels)\n",
    "    feature_train_data = feature_preds_train[remove_nan]\n",
    "    feature_train_labels = feature_train_labels[remove_nan]\n",
    "    feature_test_labels = test_labels[:,i]\n",
    "    remove_nan = ~np.isnan(feature_test_labels)\n",
    "    feature_test_labels = feature_test_labels[remove_nan]\n",
    "    feature_test_data = feature_preds_test[remove_nan]\n",
    "    feature_nn = NNet(feat_num, 300)\n",
    "    feature_nn.fit(feature_train_data, feature_train_labels)\n",
    "    feature_pred_test = feature_nn.pred(feature_test_data)\n",
    "#     feature_svm = svm.SVR(C=.0001)\n",
    "#     feature_svm.fit(feature_train_data, feature_train_labels)\n",
    "#     feature_pred_test = feature_svm.predict(feature_test_data)\n",
    "    feature_rmse_cntxt = np.sqrt(mean_squared_error(feature_test_labels, feature_pred_test))\n",
    "    feature_nnets_cntxt.append(feature_nn)\n",
    "#     feature_preds_test_cntxt.append(feat_pred_test)\n",
    "    feature_rmses_cntxt.append(feature_rmse_cntxt)\n",
    "    i += 1\n",
    "    \n",
    "print feature_rmses_indpndt\n",
    "print feature_rmses_cntxt\n",
    "print np.mean(feature_rmses_indpndt)\n",
    "print np.mean(feature_rmses_cntxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.61839384775\n",
      "3.58597021352\n"
     ]
    }
   ],
   "source": [
    "# print len(feature_test_labels)\n",
    "# print len(feature_pred_test)\n",
    "# print len(test_data)\n",
    "# print len(test_labels)\n",
    "# print len(test_labels[:,i])\n",
    "# print feature_preds_train.shape\n",
    "# print feature_pred_train[:,0].shape\n",
    "# print feature_rmses_indpndt\n",
    "# print feature_rmse_indpndt\n",
    "# print feature_pred_test*255\n",
    "# print feature_test_labels\n",
    "# print feature_rmse_cntxt\n",
    "print np.mean(feature_rmses_indpndt)\n",
    "print np.mean(feature_rmses_cntxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
